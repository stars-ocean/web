<!DOCTYPE html>
<!-- copy right: Cong Luo -->
<html>
<head>
	<script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>

	<script>
	</script>
	<style>

	.sidenav {
  width: 110px;
  position: fixed;
  z-index: 1;
  top: 30px;
  left: 10px;
  background: white;
  overflow-x: hidden;
  padding: 8px 0;
	font-size: 11px;
}

.sidenav a {
  padding: 3px 8px 6px 16px;
  text-decoration: none;
  font-size: 11px;
  color: #aaaaaa;
  display: block;
	align: left;
}

.sidenav a:hover {
  color: ##3176FE;
}



	body {
		line-height: 1.5;
		padding: 20px;
		box-shadow: 5px 5px 5px #aaaaaa;
	  text-align: justify;
	  color: #555555;
	  font-family: Arial, Helvetica, sans-serif;
		font-size:16px;
	  max-width: 650px;
	  min-width: 650px;
	  width: auto;
	  width:650px;
	  margin: auto;
	  bottom: 10px;
	  background-repeat: no-repeat; /* Do not repeat the image */
	  background-image: url('img/bg.jpg');
	  background-size: cover;
	  top: 40px;
	  z-index: -1;
	  display: block;
	}

	.color1 {
	  color: #99ccff;
	  font-weight: bold;

	}
	.color2 {
	  color: #E4974E;
	}

	a {
	  color: ##377ED9;
	}

	a:hover {
	  color: #222222;
	}

	code{
		background-color: #DAE7F8;
	}


	img{
		display: block;
		margin-left: auto;
		margin-right: auto;
	}

	.article{
	  margin: auto;
	  max-width: 500px;
	  margin-top:10px;
	  font-weight: bold;
	  font-size: 14px;
	  pad: 3px;
	}

	.article:hover {
	  color: #333333;
	  background-color: #eeeeee;
	  transition-delay: 0.02s;
	}

	.list {
	  margin: auto;
	  max-width: 500px;
	  margin-top:10px;
	}

	.container{
	  margin-top:120px;
	}

	.prettyprint{
		font-size: 13px;
		background-color: #EFF4FF;
	}

	.cap{
		max-width: 500px;
		width: 500px;
		font-size: 15px;
		font-style: italic;
		margin: auto;
	}

	#titlebar{
	  opacity: 1.0;
	  z-index: 10;
	  cursor: grab;
	  background-image: linear-gradient(#e5e4e5, #cecece);
		max-width: 600px;
	  min-width: 600px;
	  height: 22px;
	  border-radius: 0px;
	}



	#frame{
	  position: absolute;
	  margin-top: 2px;
	  background-color: #000000;
	  opacity: 0.8;
	  max-width: 600px;
	  min-width: 600px;
	  height: 350px;
	  border-radius: 5px;
	  z-index: 9;
	  font-size: 12px;
	  text-align: left;
	}

	#courtesy{
	  font-size: 12px;
		font-style: italic;
	}

	</style>
</head>
<body>

	<div class="sidenav">
	  <a href="#bg">Background</a>
	  <a href="#md">Method / result</a>
	  <a href="#th">Some thoughts</a>
	</div>



<h1>
	Leverage Parallel Performance for CPU based PyTorch Inferencing
</h1>

<h4><a name='bg'>Some backgrounds</a></h4>

<p>
Recently I've been informed by a colleague in a big project that one of my deep learning implementations for HPC has been requested and used to help a computing center (LRZ: Leibniz-Rechenzentrum) to evaluate / benchmark their next generation computating nodes (SuperMUC-NG phase 2). So I guess it worth to share this method, here are some backgrounds: although GPU is already the main computational power for deep learning tasks, for big data oriented scientific applications, the availability of the industrial grade GPU cards (e.g. Nvidia P100/V100) in most of the high performance computing centers (such as LRZ or JSC JÃ¼lich Supercomputing Centre) is still far lower than the typical CPU based computational nodes. Last year I had to translate my GPU basd Pytorch inference code into CPU based, the target machine is the LRZ's SuperMUC-NG thin node with 48 physical cores and 96 GB ram, as this type of node is the most common type in LRZ.
</p>


<p>
 A troubling issue is the low efficiency of the CPU version, I expect ~ 50% percent speed drop on a high-end 48 cores node compared with a Nvidia V100, but it runs far slower than that. More specificailly: adding more CPUs to get involved in the computation wouldn't help much, put all 48 CPUs into computation would only get you ~ 4 - 8 speed up (depends on CNN architecture), via <code>htop</code> you can see that all CPUs are indeed in computation, the saturation effect is too early. I carried out the experiments at system level: launching job in fixing CPU affinity way and setting the NUMA (non-uniform memory access), the results are not particularly promising, none of the methods got me more than 5% of the performance increment. I thought about this, one possible reason could be that affinity / NUMA will facilitate the performance for simple computation, but for the massive matrix manipulation, they probably will not be able to contribute that much. I grabed a picture from PyTorch official docs <a href="https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html?highlight=cpu%20threading">here</a>
, which seems to be a common issue. </p>
	<img src="https://pytorch.org/docs/stable/_images/cpu_threading_runtimes.svg" width="600">

<p>
	In this page I'd like to share the code level method for solving this issue. Please note it is aiming at improving the performance for multi-cores parallelization in a shared memory system, instead of the single-core, as single-core performance is a rather well solved issue (e.g. MKL-DNN, C++ based solution). If you are experiencing the same issue while working on inferencing very high resolution images (e.g. satellite observations) with PyTorch CPU version, this should help you.
</p>


<h4><a name='md'>Method and result</a></h4>

<p>
First of all, let's take a look at the software configurations:</p>

<pre class="prettyprint">Python 3.6.8
PyTorch (version 1.3.0) built with :
- GCC 7.3
- Intel MKL 2019.04 for Intel 64 architecture
- Intel MKL-DNN 0.20.5
- OpenMP 4.5
- NNPACK enabled
- BLAS = MKL
</pre>

and some specs of the CPU of target machine (SuperMUC NG thin node at LRZ) with 96 GB ram:
<pre class="prettyprint">
model name      : Intel(R) Xeon(R) Platinum 8174 CPU @ 3.10GHz
stepping        : 4
cache size      : 33792 KB
siblings        : 48
</pre>

so that the PyTorch is OpenMP and MKL-DNN built instead of the CUDA	based, meaning it is CPU instead of GPU based. Assume we are about to inference on a high resolution satellite observation image, let's say ~ 30,000 x 20,000 pixels (yes that large! as I don't have the copy right of the original satellite image, so I cannot post it here, but you can imagin &#128516;), as mentioned previously if we purely rely on the OpenMP (which is the default parallization) with few system level tricks, we woulndn't get much performance speed up even put all CPUs into the job. So from now on we completely give up the OpenMP by editing the corresponding environment variables in the python script and set the number of PyTorch threads to 1 as follow, in case you have set the CPU affinity/binding previously, you should also remove it.
<pre class="prettyprint">
os.environ['OMP_NUM_THREADS'] = '1'
torch.set_num_threads(1)
</pre>

<p>OK, now we enter the world of customization for our parallization code. Assume we would like to use all CPUs, a very natural mind came to me is spawning 48 processes for this single job, so that each process and its corresponding memory space is independent (yes, sounds like CPU-RAM affinity). After reading the image into numpy array, we split the array into 48 partitions, and arrange each partition to a corresponding CPU.</p>

<img src="img/cpu_ram.png" width="300">
<div class='cap'>
This image demonstrates the partition of the image, color indicates the index of CPU [0-47], the black area indicates where no valid in the original image.
</div>

<p>
	An important point is spawning multiple processes (memory isolated) is not identical to multithreading (memory shared within parent process), as we need to pin each memory space for a single CPU (otherwise we will not gain higher performance than OpenMP), there are handful ways for implementation, but the python <code>map</code> based methods seems not working for me. I suggest to write the PyTorch inference code (mainly the looping part) as a function meanwhile spawn it into 48 process by using <code>multiprocessing.Process()</code> method.
</p>


<pre class="prettyprint">
import psutil
import torch.nn.functional as F
import multiprocessing as mp

def model_IO_cpu_pin(cood, pubobj, model, affinity):
    proc = psutil.Process()
    proc.cpu_affinity(affinity)
    nbatch = len(cood)
    with torch.no_grad():
        for i in range(nbatch):
            tomod = from_cood_2_sampbatch(cood[i]) # I defined for batch array
            tomod = torch.from_numpy(tomod)
            tomod = F.interpolate(tomod, size=(224,224))
            ......
if __name__ == '__main__':
    allprocs = []
    for cpu in range(48):
    affi = dict(affinity=[cpu])
    p = mp.Process(target=model_IO_cpu_pin, args= (coodcpu[cpu], \
        zeros_lcz_map_sm, model, ), kwargs=affi)
    p.start()
    allprocs.append(p)
    for p in allprocs:
        p.join()
</pre>

<p>The code clause above basically implement the isolated process generation for each physical CPU, just in case you are curious about the <code>pubobj</code> in the code, it is a publically writeable object for each process, I use the <code>numpy.ctypeslib</code> to create the ctype array to pass it as the argument.

So back to our topic, what exactly happened when we run it on the target machine ? I put a <code>htop</code> screenshot here, We can see that indeed 48 processes have been allocated for each physical CPU. </p>

<img src="img/htop.png" width="400">
<div class='cap'>
You might wonder why another half of the slots are in sleep, it is because another half are actually the hyper threadings, e.g.  CPU[76] is the hyper threading of CPU[28]. Running one process on one physical core is exactly what we need, so that the CPU running time is 100% concentrated on 1 task. And it seems the task can actually go around, however it only switch within the same physical core (for security reason, I masked the host info).
</div>


<p>
Before revealing the final result,  I would like to introduce some computational parts, so from the input data, I extracted ~ 4E6 patches and interpolate them to 224 x 224 pixels, then I use the Resnet18 (with customized FC class number) to inference these patches in MKL-DNN domain and measure the running time to evaluate the performance for different number of CPUs. The same method also has been applied to the default OpenMP based solution. The full benchmark can be seen below.
</p>


<img src="img/cpu-benchmark.png" width="400">
<p>
Isn't the result amazing! The saturation effects sarted with 16 CPUs for the default OpenMP solution, eventually there is only ~ 4 x speed up compared with single CPU, meaning you basically waste a tremendous amount of computational resources if you simply specify <code>torch.set_thread_num(nCPU)</code>. However for the proposed menthod, you get nearly linear speed up, the highest performance is ~ 10 x faster than the default solution. Our design and implementation worked !
</p>

<p>
 Just in case you are interested, the sicientific result of this computation is the classification map for a remote area near Tokyo.
</p>

<img src="img/tk_demo.png" width="350">

<h4><a name='th'>Some thoughts</a></h4>


<p>
I am actually rather curious about the deeper reason why the Pytorch with OpenMP somehow is significant slower than the theoretical max, or it is just a phenomenon that only happen to CNN based architecture, as another CNN architecture used in our project also exhibits a similar behavior. During this experiments, I also translate the code into with C++ using the LibTorch to exam the single core performance, which runs roughly at the same speed as Python version, I guess maybe the C++ and Python are actually using the same background,  if I have time, I may also write another article for exploring the methods to imporve single core performance for CPU node.
</p>


<p id='courtesy'>
	Researcher / content creator: Cong Luo &#128516; @ TUM.
</p>
</body>
</html>
